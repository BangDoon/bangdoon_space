{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCIJ8IvaMg3s"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://velog.io/@gibonki77/ResNetwithPyTorch?fbclid=IwAR2uQa5gtIAXk_D-Sd4FLhQmLIntwvAgeSttG_cT-nxzk5sRXIU6nnF1m2g\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms  # 이미지 변환\n",
    "\n",
    "class Conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation=True):\n",
    "        super(Conv_block, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.activation:\n",
    "            return self.batch_norm(self.conv(x))\n",
    "        \n",
    "        return self.relu(self.batch_norm(self.conv(x)))\n",
    "        \n",
    "        \n",
    "class Res_block(nn.Module):\n",
    "    def __init__(self, in_channels, red_channels, out_channels, is_plain=False):\n",
    "        super(Res_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.is_plain = is_plain\n",
    "        \n",
    "        if in_channels == 64:\n",
    "            self.conv_seq = nn.Seqential(\n",
    "                                    Conv_block(in_channels, red_channels, kernel_size = 1, padding=0)\n",
    "                                    Conv_block(red_channels, red_channels, kernel_size = 3, padding =1)\n",
    "                                    Conv_block(red_channels, out_channels, activation=False, kernel_size=1, padding=0)\n",
    "            )\n",
    "        elif in_channels == out_channels:\n",
    "            self.conv_seq = nn.Seqential(\n",
    "                                    Conv_block(in_channels, red_channels, kernel_size = 1, padding=0)\n",
    "                                    Conv_block(red_channels, red_channels, kernel_size = 3, padding =1)\n",
    "                                    Conv_block(red_channels, out_channels, activation=False, kernel_size=1, padding=0)\n",
    "            )\n",
    "            self.iden = nn.Identity()\n",
    "        else :\n",
    "            self.conv_seq = nn.Seqential(\n",
    "                                    Conv_block(in_channels, red_channels, kernel_size = 1, padding=0, stride=2)\n",
    "                                    Conv_block(red_channels, red_channels, kernel_size = 3, padding =1)\n",
    "                                    Conv_block(red_channels, out_channels, activation=False, kernel_size=1, padding=0)\n",
    "            )\n",
    "            self.iden = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv_seq(x)\n",
    "        if self.is_plain:\n",
    "            x = y\n",
    "        else:\n",
    "            x = y + self.iden(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1000, is_plain=False):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv_block(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2_x = nn.Sequential(\n",
    "                                Res_block(64, 64, 256, is_plain),\n",
    "                                Res_block(256, 64, 256, is_plain),\n",
    "                                Res_block(256, 64, 256, is_plain)      \n",
    "        )\n",
    "        self.conv3_x = nn.Sequential(\n",
    "                                Res_block(256, 128, 512, is_plain),\n",
    "                                Res_block(512, 128, 512, is_plain),\n",
    "                                Res_block(512, 128, 512, is_plain),\n",
    "                                Res_block(512, 128, 512, is_plain)      \n",
    "        )\n",
    "        self.conv4_x = nn.Sequential(\n",
    "                                Res_block(512, 256, 1024, is_plain),\n",
    "                                Res_block(1024, 256, 1024, is_plain),\n",
    "                                Res_block(1024, 256, 1024, is_plain),\n",
    "                                Res_block(1024, 256, 1024, is_plain),\n",
    "                                Res_block(1024, 256, 1024, is_plain),\n",
    "                                Res_block(1024, 256, 1024, is_plain),      \n",
    "        )\n",
    "        self.conv5_x = nn.Sequential(\n",
    "                                Res_block(1024, 512, 2048, is_plain),\n",
    "                                Res_block(1024, 512, 2048, is_plain),\n",
    "                                Res_block(1024, 512, 2048, is_plain),      \n",
    "        )\n",
    "        \n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size = 7, stride=1)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2_x(x)\n",
    "        x = self.conv3_x(x)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary as summary_\n",
    "\n",
    "def build_resnet(input_shape=(3,225,225), is_50 = True, is_plain=False):\n",
    "    \n",
    "    x = torch.randn(2, *input_shape).to(device)\n",
    "    \n",
    "    if is_50 :\n",
    "        model = ResNet(is_plain=is_plain).to(device)\n",
    "        \n",
    "        assert = model(x).shape == torch.Size([2, model.num_classes])\n",
    "        \n",
    "        if is_plain == False:\n",
    "            print(\"ResNet 50\")\n",
    "        if is_plain == True:\n",
    "            print(\"PlainNet50 \")\n",
    "            \n",
    "        print(summary_(model, (3,225,225), batch_size = 2))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    model = ResNet_34(is_plain).to(device)\n",
    "    \n",
    "    assert model(x).shape == torch.Size([2, model.num_classes])\n",
    "    \n",
    "    if is_plain == False:\n",
    "        print(\"ResNet 34 \")\n",
    "    \n",
    "    if is_plain == True:\n",
    "        print(\"PlainNet 34\")\n",
    "    \n",
    "    print(summary_(model, (3,225,255), batch_size = 2))\n",
    "    \n",
    "    return model\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(\"using {}\".format(device))\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "model = build_resnet().to(device)\n",
    "\n",
    "loss = nn.MSELoss(reduction = 'sum')\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer):\n",
    "    \n",
    "    best_model_weight = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    \n",
    "    EPOCH = 100\n",
    "    for epoch in range(EPOCH):\n",
    "        print(\"epoch : {}/{} \\n\".format(epoch,EPOCH))\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            current_loss = 0\n",
    "            current_correct = 0\n",
    "            \n",
    "            for inputs, labels in database['phase']:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs,1)\n",
    "                    loss = loss_func(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                current_loss += loss.item() * inputs.size(0)\n",
    "                current_correct += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = current_loss / database[phase]\n",
    "            epoch_acc = current_correct.double() / database[phase]\n",
    "            \n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_weight = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_model_wight)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def train(model):\n",
    "    model.train(True)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "    \n",
    "    for _ in range(batches):\n",
    "        inputs = dataset\n",
    "    \n",
    "        optimizer.zero_grad()  # 변화도 초기화 (0)\n",
    "        outputs = model(inputs.to(device))  # 순전파 학습\n",
    "\n",
    "        labels = labels.to(outputs.device)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)  # 결과 해석\n",
    "        loss.backward() # 역전파 단계 / 해당 단계를 거치면 inputs.grad에 변화도(gradient) 저장\n",
    "        \n",
    "        optimizer.step()  # 경사 하강법 (gradient descent)\n",
    "    \n",
    "    \n",
    "def test(model, dataset, loss, optimizer):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():   # 변화도 계산 안함. eval이나 test용으로 사용\n",
    "        for i in range(dataset):\n",
    "            data, targets = get_batch(dataset, i)\n",
    "            \n",
    "# 매개변수 고정\n",
    "model = Net(require_grad = True) # require_grad : True = 변화도 계산 / False = 변화도 계산 X (전이학습 용)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(126,10) # 분류기 layer / 10개의 항목 분류\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = './*.pt'\n",
    "# Path = './*.pth'\n",
    "\n",
    "## state_dict 활용 모델 저장 & 호출 (모델의 매개변수만 저장, 사용할 땐 모델 선언 후 인수로 투입)\n",
    "torch.save(model.state_dict(), Path)\n",
    "\n",
    "model = build_resnet()\n",
    "model.load_state_dict(torch.load(Path))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "## 전체 모델 저장 & 호출 ()\n",
    "torch.save(model, Path)\n",
    "\n",
    "model = torch.load(Path)\n",
    "model.eval()\n",
    "\n",
    "## checkpoint 저장 & 호출\n",
    "EPOCH = 100\n",
    "LOSS = 0.01\n",
    "\n",
    "torch.save({\n",
    "    'epoch':EPOCH,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict' : optimizer.state_dict(),\n",
    "    'loss':LOSS,\n",
    "}, Path)\n",
    "\n",
    "model = build_resnet()\n",
    "optimizer = optim.Adam(model.parameter(), lr = 0.001)\n",
    "\n",
    "checkpoint = torch.load(Path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "model.train()\n",
    "\n",
    "## 여러 모델을 하나의 파일에 저장 & 호출\n",
    "\n",
    "model_A = build_resnet()\n",
    "model_B = build_resnet()\n",
    "\n",
    "optimizer_A = optim.Adam(model_A.parameters(), lr = 0.001)\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr = 0.001)\n",
    "\n",
    "torch.save({\n",
    "    'model_A_state_dict': model_A.state_dict(),\n",
    "    'model_B_state_dict': model_B.state_dict(),\n",
    "    'optimizer_A_state_dict': optimizer_A.state_dict(),\n",
    "    'optimizer_B_state_dict': optimizer_B.state_dict()\n",
    "}, Path)\n",
    "\n",
    "## 부분적인 모델 파라미터 호출 예시\n",
    "\n",
    "model_A = Net_A()\n",
    "torch.save(model_A.state_dict(), Path)\n",
    "\n",
    "model_B = Net_B()\n",
    "model_B = load_state_dict(torch.load(Path), strict=False)\n",
    "\n",
    "## 저장한 곳 -> 불러오는 곳 (GPU -> CPU / GPU -> GPU / CPU -> GPU)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(Path, map_location = device))\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(Path))\n",
    "model.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(Path, map_location=\"cuda:0\"))\n",
    "model.to(device)\n",
    "\n",
    "## 이미지 변환 함수\n",
    "from PIL import Image\n",
    "\n",
    "def transform_image(image):\n",
    "    input_transforms = [transforms.Resize(255),\n",
    "                        transforms.CenterCrop(225),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([]),        \n",
    "    ]\n",
    "    my_transforms = transforms.Compose(input_transforms)\n",
    "    \n",
    "    img = Image.open(image)\n",
    "#     img = Imga.open(io.BytesIO(image)) # 이미지를 bytes 단위로 읽음\n",
    "    \n",
    "    t_img = my_transforms(img)\n",
    "    t_img.unsqueeze_(0)\n",
    "    \n",
    "    return t_img\n",
    "\n",
    "## 예측 결과, class 일치 여부 확인\n",
    "def render_prediction(prediction_idx):\n",
    "    str_idx = str(prediction_idx)\n",
    "    class_name = 'unknown'\n",
    "    \n",
    "    if img_class_list is not None:\n",
    "        if str_idx in img_class_list is not None:\n",
    "            class_name = image_class_list[str_idx][1]\n",
    "            \n",
    "    return prediction_idx, class_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### encoder & decoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### torch GAN\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 배포\n",
    "\n",
    "model = Net()\n",
    "model.eval()\n",
    "\n",
    "# 성능을 동일하게 유지하면서 모델의 크기를 줄이기 위한 모델 양자화(quantization)\n",
    "# LSTM 등 시계열 모델은 동적 양자화에 최적화\n",
    "\n",
    "backend = \"fbgemm\"\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec = {torch.nn.Linear}, dtype = torch.qint8)\n",
    "\n",
    "# 양자화된 모델을 모바일에서 실행 가능할 수 있도록 TorchScript 형식으로 변환\n",
    "# 모델에 제어흐름이 있는 경우는 script, 제어가 없는 간단한 모델일 경우 trace\n",
    "\n",
    "\n",
    "# scripted_quantized_model = torch.jit.trace(quantized_model)\n",
    "scripted_quantized_model = torch.jit.script(quantized_model)\n",
    "scripted_quantized_model.save(Path + \"/scripted_qauntized_model.pt\")\n",
    "\n",
    "\n",
    "# 모바일 최적화\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
    "optimized_scripted_quantized_model.save(Path + \"/optimized_scripted_qauntized_model.pt\")\n",
    "\n",
    "\n",
    "# 라이트 인터프리터 적용 (용량 감소로 속도 증가 도모)\n",
    "optimized_scripted_quantized_model._save_for_lite_interpreter(\"lite_optimized_scripted_qauntized_model.pt\")\n",
    "\n",
    "\n",
    "# 테스트 코드 / 보통 모바일에서는 cpu를 사용하니 cpu 성능으로 테스트\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as profile:\n",
    "    output = optimized_scripted_quantized_model(input_image)\n",
    "    \n",
    "print(\"{:.2f}\".format(profile.self_cpu_time_total)/1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 가지치기\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "module = model.conv1\n",
    "\n",
    "prune.random_unstructured(module, name = \"test\", amount=0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pytorch 모델을 C++ 에서 로드하기\n",
    "\n",
    "# LibTorch 사용\n",
    "\n",
    "include <torch/script.h>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flask 배포 (Rest API)\n",
    "\n",
    "from flask import Flask, jsonify\n",
    "from flask import request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "model = Net()\n",
    "model.eval()\n",
    "\n",
    "## 예측\n",
    "\n",
    "def get_prediction(image):\n",
    "    tensor = transform_image(image)\n",
    "    outputs = model.forward(tensor)\n",
    "    _, y_hat = outputs.max(1)\n",
    "    \n",
    "    image_class = y_hat.item()\n",
    "\n",
    "    return image_class\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if request.method == 'POST':\n",
    "        file = request.files['file']\n",
    "        \n",
    "        image = file.read()\n",
    "        \n",
    "        class_id, class_name = get_prediction(image)\n",
    "    \n",
    "        return jsonify({'class_id': 'id', 'class_name':'name'})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n",
    "    \n",
    "# $ FLASK_ENV = development FLASK_APP = app.py flask run\n",
    "\n",
    "# 잘못된 형식의 파일을 전송할 경우를 대비해 예외 처리가 필요함.\n",
    "# 모델이 파일의 데이터 자체를 인식 못할 경우가 발생하니 이를 방지해야함.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module sharing\n",
    "\n",
    "class Module_A(nn.Module):\n",
    "    def __init__(self, Module_S):\n",
    "        super(Module_A, self).__init__()\n",
    "        \n",
    "        self.Module_S = Module_S()\n",
    "        \n",
    "    def forward(self, input_data ):\n",
    "        X1 = self.Module_S(input_data)\n",
    "        X2 = \n",
    "        \n",
    "        X = torch.cat((X1, X2), dim=1)\n",
    "        X = F.relu(~~)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "class Module_B(nn.Module):\n",
    "    def __init__(self, Module_S):\n",
    "        super(Module_B, self).__init__()\n",
    "        \n",
    "        self.Module_S = Module_S()\n",
    "        \n",
    "    def forward(self, input_data ):\n",
    "        X1 = self.Module_S(input_data)\n",
    "        X2 = \n",
    "        \n",
    "        X = torch.cat((X1, X2), dim=1)\n",
    "        X = F.relu(~~)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    \n",
    "class Module_S(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Module_S, self).__init__()\n",
    "        \n",
    "    def forward(self, input, ):\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "Shared_Module = Module_S()\n",
    "\n",
    "Model_A = Module_A(Module_S = Shared_Module)\n",
    "Model_B = Module_B(Module_S = Shared_Module)\n",
    "\n",
    "def train(Model_A, Model_B, dataset):\n",
    "    global Shared_Module\n",
    "    \n",
    "    EPOCH = 100\n",
    "    \n",
    "    For_A, For_B = dataset\n",
    "    \n",
    "    for i in range(dataset):\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            inputs, targets = get_batch(For_A)\n",
    "            output = Model_A.train(inputs)\n",
    "            \n",
    "            loss_A = loss_function(output, targets)\n",
    "            loss_A.backward()\n",
    "            \n",
    "        else:\n",
    "            inputs, targets = get_batch(For_B)\n",
    "            output = Model_B.train(inputs, tragets)\n",
    "            \n",
    "            loss_B = loss_function(output, targets)\n",
    "            loss_B.backward()\n",
    "            \n",
    "    Shared_Module.save(Path)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "[0 1 2 ... 8 9 8]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1797, 64)\n",
      "(1797,)\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1797, 64])\n",
      "torch.Size([1797])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import torch\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1797, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_116760/1644322465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[0].shape)\n",
    "print(X[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "module_sharing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sharing",
   "language": "python",
   "name": "sharing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
